---
title: "New Deposit Product Survey Analysis"
author: "Yan Naing Oo"
date: "`r Sys.Date()`"
output: word_document
---

# Load Packages

```{r}
library(readxl)
library(tm)
library(dplyr)
library(SnowballC)
library(tokenizers)
library(quanteda, quietly = TRUE)
library(wordcloud)
library(tidytext)

```

# import excel file

```{r}
data <- read_excel("C:\\Users\\yoo\\Documents\\Yang\\Product_Analysis\\New Deposit Products Design Project all staff data.xlsx" ,  sheet = "Sheet1")
data_all <- read_excel("C:\\Users\\yoo\\Documents\\Yang\\Product_Analysis\\New Deposit Products Design Project all staff data.xlsx" ,  sheet = "All")

# Assuming 'data' is the data frame containing the text
q1_text <- data$`What are our most popular deposit products?`
q2_text <- data$`Why are those deposit products popular?`
q3_text <- data$`What did we do to make them popular?`
q4_text <- data$`Who are our competitors?`
q5_text <- data$`Why are they good?Â `
q6_text <- data$`What have our customers asked about?`
all_text <- data_all$tweet_text


```

# Define the clean_corpus function to preprocess text

```{r}

clean_corpus <- function(text){
  text %>%
    tolower() %>%
    removePunctuation() %>%
    removeNumbers() %>%
    tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
    tokens_remove(pattern = c(stopwords("en"), "and", "i", "me", "you", 
                             "your", "are", "was", "us", "a", "they", "their", "theirs", 
                             "he", "him", "his", "na")) %>%
    tokens_ngrams(n = 2, concatenator = " ") 
    #tokens_replace(pattern = c("relat", "busi"), replacement = c("relate", "business")) %>%
    # Use blank space " " concatenator to join words in bigrams
     
  
}


```

# Preprocess the text and create tokens

```{r}

corpus_q1 <- q1_text %>%
  clean_corpus()
corpus_q2 <- q2_text %>%
  clean_corpus()
corpus_q3 <- q3_text %>%
  clean_corpus()
corpus_q4 <- q4_text %>%
  clean_corpus()
corpus_q5 <- q5_text %>%
   clean_corpus()
corpus_q6 <- q6_text %>%
   clean_corpus()
corpus_all <- all_text %>%
   clean_corpus()

#corpus_all <- c(q1_text, q2_text, q3_text, q4_text, q5_text, q6_text) %>%
  #clean_corpus()

```

```{r}
# Create a Document-Term Matrix (DTM)
dtm_q1 <- dfm(corpus_q1)
dtm_q2 <- dfm(corpus_q2)
dtm_q3 <- dfm(corpus_q3)
dtm_q4 <- dfm(corpus_q4)
dtm_q5 <- dfm(corpus_q5)
dtm_q6 <- dfm(corpus_q6)
dtm_all <- dfm(corpus_all)


# Sum over columns and get total count for each term
freq_terms_q1 <- colSums(dtm_q1)
freq_terms_q2 <- colSums(dtm_q2)
freq_terms_q3 <- colSums(dtm_q3)
freq_terms_q4 <- colSums(dtm_q4)
freq_terms_q5 <- colSums(dtm_q5)
freq_terms_q6 <- colSums(dtm_q6)
freq_terms_all <- colSums(dtm_all)

# Sort terms by frequency in descending order
sort_terms_q1 <- order(freq_terms_q1, decreasing = TRUE)
sort_terms_q2 <- order(freq_terms_q2, decreasing = TRUE)
sort_terms_q3 <- order(freq_terms_q3, decreasing = TRUE)
sort_terms_q4 <- order(freq_terms_q4, decreasing = TRUE)
sort_terms_q5 <- order(freq_terms_q5, decreasing = TRUE)
sort_terms_q6 <- order(freq_terms_q6, decreasing = TRUE)
sort_terms_all <- order(freq_terms_all, decreasing = TRUE)

# Get the top 10 most frequent bigrams
top_terms_q1 <- names(freq_terms_q1[sort_terms_q1[1:10]])
top_terms_q2 <- names(freq_terms_q2[sort_terms_q2[1:10]])
top_terms_q3 <- names(freq_terms_q3[sort_terms_q3[1:10]])
top_terms_q4 <- names(freq_terms_q4[sort_terms_q4[1:10]])
top_terms_q5 <- names(freq_terms_q5[sort_terms_q5[1:10]])
top_terms_q6 <- names(freq_terms_q6[sort_terms_q6[1:10]])

```

# Print the top 10 most frequent terms

```{r}

print(head(top_terms_q1, 10))
print(head(top_terms_q2, 10))
print(head(top_terms_q3, 10))
print(head(top_terms_q4, 10))
print(head(top_terms_q5, 10))
print(head(top_terms_q6, 10))
```

```{r}
# Calculate percentages for the top 10 bigrams
percentages_q1 <- prop.table(freq_terms_q1[sort_terms_q1[1:10]]) * 100
percentages_q2 <- prop.table(freq_terms_q2[sort_terms_q2[1:10]]) * 100
percentages_q3 <- prop.table(freq_terms_q3[sort_terms_q3[1:10]]) * 100
percentages_q4 <- prop.table(freq_terms_q4[sort_terms_q4[1:10]]) * 100
percentages_q5 <- prop.table(freq_terms_q5[sort_terms_q5[1:10]]) * 100
percentages_q6 <- prop.table(freq_terms_q6[sort_terms_q6[1:10]]) * 100


# Create a color palette with light blue for the bars
colors1 <- rep("lightblue", length(top_terms_q1))
colors2 <- rep("lightblue", length(top_terms_q2))
colors3 <- rep("lightblue", length(top_terms_q3))
colors4 <- rep("lightblue", length(top_terms_q4))
colors5 <- rep("lightblue", length(top_terms_q5))
colors6 <- rep("lightblue", length(top_terms_q6))

# Set Survey questions
question_titles <- c(
  "What are our most popular deposit products",
  "Why are those deposit products popular",
  "What did we do to make them popular",
  "Who are our competitors",
  "Why are they good",
  "What have our customers asked about"
)
# Set the plot width to avoid truncating the legend
par(mar = c(5, 8, 4, 3))  # Adjust the margins to make room for the legend

set.seed(100)

# Create a list of colors for each dataset
colors_list <- list("lightblue", "lightblue", "lightblue", "lightblue", "lightblue", "lightblue")

# Loop over each dataset to create the barplot
for (i in 1:6) {
  # Get the current dataset's frequency, sorted terms, top terms, and percentages
  freq <- get(paste0("freq_terms_q", i))[get(paste0("sort_terms_q", i))[1:10]]
  top_terms <- get(paste0("top_terms_q", i))
  percentages <- get(paste0("percentages_q", i))
  colors <- colors_list[[i]]  # Get the corresponding color vector

  # Create the barplot for the current dataset
  barplot(freq, names.arg = top_terms, horiz = TRUE, las = 1,
          
          col = colors, main = question_titles[i],
          xlab = "Frequency", ylab = "", xlim = c(0, max(freq)))
  
  # Add percentages as labels to the bars
  text(x = freq, y = seq_along(top_terms),
       labels = paste0(round(percentages, 1), "%"), pos = 2.5)
}

```

# Create a word cloud with all the terms

```{r}

# Display the top 200 most frequent terms
wordcloud_terms <- names(freq_terms_all[sort_terms_all[1:200]])  
# Define colors for the word cloud
my_colors <- c("blue", "green", "red", "purple", "orange", "brown", "pink", 
               "gray", "yellow", "cyan")

set.seed(100)
# Create a word cloud with the selected terms
wordcloud(words = wordcloud_terms, freq = freq_terms_all[sort_terms_all],
          scale = c(1.5, 0.5),
          random.order = FALSE, rot.per = 0.5, colors = brewer.pal(8, "Dark2"))
# Add a title to the word cloud plot
title(main = "Overview of New Deposit Product", cex.main = 1.5, line = 1.5)
```
